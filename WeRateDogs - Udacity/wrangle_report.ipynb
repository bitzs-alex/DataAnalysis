{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I went through the three steps of the wrangling process step by step, and I will briefly discuss each process I went through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each data has its source, I first identified what my data sources are and what are the things I need to have to gather the data. In this case, I have three datasets from three different sources. One, already given CSV file called `twitter-archive-enhanced.csv. ` Two, a link to a file in Udacity’s server. Three, a Twitter API that requires an access token and key to send requests and receive data. I applied to Twitter’s Developers site to get an access token and key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the data sources, I jumped to the first stage of the Data Wrangling process, i.e., Gathering the Data. To collect the data stored in Udacity’s server, I used Python’s requests module to download the file and store it locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Twitter API requires the tweet IDs to request more data about the tweet, I grabbed all the tweet IDs from ` twitter-archive-enhanced.csv ` and used tweepy Python module iteratively loop through the tweet IDS and request more data about each tweet. By doing so, I collected 2327 tweets data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step in wrangling is Assessment, I employed both visual and programmatic assessment to find both quality and tidiness issues and document them for each dataset. These are the issues I encountered and documented during the assessment stage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`tweets_archive` Dataset\n",
    "1. Clean retweets (reply to a user or tweet) uring `in_reply_to_status_id` and/or `in_reply_to_user_id`\n",
    "2. `timestamp` to `datetime\n",
    "3. Unnecessary columns like `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`\n",
    "4. Same scale and range for `rating_denominator` and `rating_numerator` columns respectively\n",
    "6. Extract the source from `source` column \n",
    "8. Incorrect names like 'a', 'an', 'very', 'the' in `name` column \n",
    "\n",
    "`image_predictions` Dataset\n",
    "1. Missing rows; it supposed to be aligned with `tweets_archive` dataset which has 2356 rows\n",
    "2. Unnecessary column `img_num`\n",
    "4. None dog predictions, i.e., both `p1_dog`, `p2_dog`, and `p3_dog` values are `False`\n",
    "\n",
    "`tweets_info` Dataset\n",
    "1. Missing rows; it supposed to be 2356 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "**Tidiness issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`tweets_archive` Dataset\n",
    "1. `dog_stage` supposed to be in single column\n",
    "2. `text` column supposed to contain only tweet text (`tweet_link` should be removed)\n",
    "\n",
    "`image_predictions` Dataset\n",
    "1. Only one prediction for dog image instead of three\n",
    "\n",
    "`All`\n",
    "1. All Datasets supposed to be merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cleaning stage, I picked the issues mentioned above one by one and fixed them. I went through the three cleaning stages while fixing the issues. I defined, coded, and tested for each case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I merged the datasets while removing unnecessary columns and stored the cleaned dataset as ` twitter_archive_master.csv. `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"ejGlV5FlVsiFmnexfUQn3ZcMa\"\n",
    "consumer_secret = \"GwpnS712W81rOX44H7hdQclEKXvETd4ydFTTztB9YkawDlEMhF\"\n",
    "access_token = \"3131341967-cEO8acOqxisafHj0onX8bAKA8eeJsfixFGJtieW\"\n",
    "access_secret = \"vuhoUKFC3l43aCfxYs1yJNCeY0Z1tlyLdREkncjSLw7wQ\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('3.10.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "884de70e66b0cc1739cd15e61889754aad2231ff1c3453754bdb55b08d20c7dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
